import re
import os
import asyncio
from playwright.async_api import async_playwright

KUNPENG_DOC_URL = "https://www.hiascend.com/zh/document?tab=document"
SAVE_PATH = "/Users/caixiaomeng/Projects/Python/DataBuilder/data/raw/ascend"


async def sanitize_filename(path):
    # ç§»é™¤æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
    path = re.sub(r'[<>:"/\\|?*]', "_", path)
    # å»æ‰ä¸¤ç«¯çš„ç©ºç™½å­—ç¬¦
    path = path.strip()
    # æ›¿æ¢ç©ºæ ¼ä¸ºä¸‹åˆ’çº¿
    path = path.replace(" ", "_")
    # é™åˆ¶æ–‡ä»¶åé•¿åº¦ï¼ˆWindows æ–‡ä»¶åæœ€å¤§é•¿åº¦ä¸º255ï¼‰
    path = path[:255]
    return path


async def get_breadcrumbs(page):
    try:
        breadcrumbs = await page.query_selector_all("span.bread-item-inner")
        breadcrumb_list = []
        for breadcrumb in breadcrumbs:
            text = await breadcrumb.inner_text()
            # ä½¿ç”¨awaitè°ƒç”¨sanitize_filename
            breadcrumb_list.append(await sanitize_filename(text.strip()))
        return breadcrumb_list[:-1]
    except Exception as e:
        print(f"[é”™è¯¯] è·å–æ¥æºå¤±è´¥: {e}")
        return []


async def save_article(page):
    breadcrumb_list = await get_breadcrumbs(page)

    if not breadcrumb_list:
        print("[è·³è¿‡]æå–æ–‡ç« æ¥æºå¤±è´¥")
        return False

    dir_name = os.path.join(*breadcrumb_list)
    full_path = os.path.join(SAVE_PATH, dir_name)

    if not os.path.exists(full_path):
        os.makedirs(full_path)
        print(f"ç›®å½• '{full_path}' å·²åˆ›å»ºã€‚")

    try:
        await page.wait_for_timeout(1000)  # ç¨³å®š DOM

        title_el = await page.query_selector("section.topic-section h1.topic-title")
        if not title_el:
            print("[è·³è¿‡] éæ–‡ç« é¡µé¢ï¼ˆæ— æ ‡é¢˜ï¼‰")
            return False

        title_text = await title_el.inner_text()
        safe_title = title_text.strip().replace("/", "-").replace("\\", "-")

        content_el = await page.query_selector(
            "section.article-content .the-article.document-article"
        )
        if content_el:
            content = await content_el.inner_html()  # åŠ ä¸Š await
            filename = os.path.join(full_path, f"{safe_title}.html")
            with open(filename, "w", encoding="utf-8") as f:
                f.write(content)
            print(f"[å·²ä¿å­˜] {filename}")
        else:
            print(f"[è·³è¿‡] æ— æ­£æ–‡å†…å®¹: {safe_title}")
        return True
    except Exception as e:
        print(f"[é”™è¯¯] ä¿å­˜æ–‡ç« å¤±è´¥: {e}")
        return False


async def process_list_item(context, page, index):
    await page.wait_for_selector(".document-section-pc .list-wrapper .list-item")
    list_items = await page.query_selector_all(
        ".document-section-pc .list-wrapper .list-item"
    )

    if index >= len(list_items):
        return

    async with context.expect_page() as new_page_info:
        await list_items[index].click()

    new_page = await new_page_info.value
    await new_page.wait_for_load_state("networkidle")

    is_article = await save_article(new_page)
    if not is_article:
        await new_page.close()
        return

    while True:
        next_btn = await new_page.query_selector("div.next-article.is-exist")
        if next_btn:
            await next_btn.click()
            await new_page.wait_for_timeout(1000)
            success = await save_article(new_page)
            if not success:
                break
        else:
            break

    await new_page.close()


"""
çˆ¬å–é²²é¹å®˜ç½‘çš„å…¨éƒ¨æ–‡æ¡£ï¼Œä»¥htmlå½¢å¼ä¿å­˜
"""


async def kunpeng_crawler():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)  # è®¾ä¸ºTrueå¯æ— å¤´è¿è¡Œ
        context = await browser.new_context()
        page = await context.new_page()

        await page.goto(KUNPENG_DOC_URL, wait_until="networkidle")

        await page.wait_for_selector(".o-pagination-pages div")
        pages = await page.query_selector_all(".o-pagination-pages div")
        last_span = await pages[-1].query_selector("span")
        max_page = int(await last_span.inner_text())
        print(f"ğŸ“„ æ€»é¡µæ•°: {max_page}")

        for current_page in range(1, max_page + 1):
            print(f"\n===== ç¬¬ {current_page} é¡µ =====")
            await page.wait_for_selector(
                ".document-section-pc .list-wrapper .list-item"
            )
            list_items = await page.query_selector_all(
                ".document-section-pc .list-wrapper .list-item"
            )
            total_items = len(list_items)

            for idx in range(total_items):
                print(f"\n-- å¤„ç†ç¬¬ {idx + 1}/{total_items} ä¸ªæ¡ç›® --")
                try:
                    await process_list_item(context, page, idx)
                except Exception as e:
                    print(f"[é”™è¯¯]å¤„ç†itemå¤±è´¥ï¼Œé”™è¯¯åŸå› æ˜¯ï¼š{e}")

            if current_page < max_page:
                next_page_btn = await page.query_selector("div.o-pagination-next")
                if next_page_btn:
                    await next_page_btn.click()
                    await page.wait_for_timeout(2000)

        await browser.close()


if __name__ == "__main__":
    asyncio.run(kunpeng_crawler())
